{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 03: Classification I\n",
    "\n",
    "In this notebook, we perform classification of Robot movements in an environment with walls using real dataset. \n",
    "\n",
    "Install the necessary libraries in the PC or in the Virtual Environment using provided Requirements.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from functions.fisher_score import fisher_index_calc\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from functions.plot_confusion_matrix import plot_confusion_matrix\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data Preprocessing\n",
    "\n",
    "Load the given data in the 'Data' folder and analyse the data and solve the following questions.\n",
    "\n",
    "1. What is the sample size ?\n",
    "2. Is data labeled? If yes print the labeles of the data.\n",
    "3. Check the features and type of data. \n",
    "4. Plot the data distribution of Features towards labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from csv to Pandas Dataframe\n",
    "\n",
    "data = np.loadtxt(\"Data/sensor_readings_24.csv\", delimiter=',', dtype=str)\n",
    "\n",
    "df = pd.DataFrame(data[:,:24], dtype=np.float64)\n",
    "df = pd.concat([df, pd.DataFrame(data[:, 24], columns=['Label'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the data by printing the sample data and its shape\n",
    "print(\"Size of the Data:\", ...)\n",
    "print (\"Sample Data:\\n\", ...)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the data distribution and check for the following.\n",
    "\n",
    "1. Is normalization required ?\n",
    "2. What do you observe from the data about data distribution and asses if data is balanced?\n",
    "3. What do you think that is needed further by analysing the data ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabulate the sample data set using describe function and analyse. \n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'Label'\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data distribution using SNS countplot\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "axis = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Feature Selection\n",
    "\n",
    "## Little bit of Theory about  Feature Selection\n",
    "Different types of Feature Selection methods:  \n",
    "<img src=\"figures/feature_selection_methods.png\">  \n",
    "Source: Medium.com\n",
    "\n",
    "#### Correlation Statistics  \n",
    "The scikit-learn library provides an implementation of most of the useful statistical measures.  \n",
    "For example:  \n",
    "1. Pearson’s Correlation Coefficient: f_regression()  \n",
    "2. ANOVA: f_classif()  \n",
    "3. Chi-Squared: chi2()  \n",
    "4. Mutual Information: mutual_info_classif() and mutual_info_regression()  \n",
    "Also, the SciPy library provides an implementation of many more statistics, such as Kendall’s tau (kendalltau) and Spearman’s rank correlation (spearmanr).\n",
    "#### Selection Method\n",
    "The scikit-learn library also provides many different filtering methods once statistics have been calculated for each input variable with the target.\n",
    "Two of the more popular methods include:\n",
    "1. Select the top k variables: SelectKBest\n",
    "2. Select the top percentile variables: SelectPercentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the following: \n",
    "1. What type of feature selection methods that are applicable for the given dataset? \n",
    "2. What do you think about the data size and how it influence the learning?\n",
    "3. Do we need large data to train the models for better results? \n",
    "4. What do you mean by large data ? Large no. of samples Vs More features ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplr way of feature selection.\n",
    "# Apply the suitable feature selection method from above description and extract the data.\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "# generate dataset\n",
    "\n",
    "# define feature selection\n",
    "fs = ...\n",
    "# apply feature selection\n",
    "df_selected_1 = ...\n",
    "print(df_selected_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Based Feature Selection using Fisher Score \n",
    "\n",
    "training_set = ... # Extract Training set without labels\n",
    "label_set = ... # Extract labels\n",
    "\n",
    "# Get the fisher scores\n",
    "fisher_scores = ...\n",
    "\n",
    "# Plot the fisher scores\n",
    "fig= plt.figure(figsize=(23, 10))\n",
    "df_fisher = pd.DataFrame({'Fisher Scores of the Features': fisher_scores})\n",
    "ax = df_fisher.plot.bar(figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you infer from the above plot ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature selection by analysing the above plot\n",
    "# Remove the features that are not significant according to your analysis.\n",
    "\n",
    "to_remove = []\n",
    "for i in range((len(fisher_scores))):\n",
    "    if ...: # Condition for data filtering\n",
    "        # we mark for removal\n",
    "        to_remove.append(i)\n",
    "\n",
    "df_selected_2 = ... # Delete the data to be removed from training_set\n",
    "df_selected_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Model Learning for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preperation for Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Train data splitting\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "labelEn = LabelEncoder()\n",
    "encoded_labels = ... # fit the lable encoder to encode lables with numerical values\n",
    "class_names = labelEn.classes_\n",
    "\n",
    "X_train, X_test, y_train, y_test = ... # perform train test split with test size 0.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementat your own KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement KNN classifier with euclideab Distance\n",
    "def mode(list):\n",
    "    return ... # Formula for mode\n",
    "def euclidean(point, data):\n",
    "    return ... # Formula for Euclidean Distance\n",
    "\n",
    "class KNeighborsClassifier:\n",
    "    def __init__(self, k=5, dist_metric=euclidean):\n",
    "        self.k = k\n",
    "        self.dist_metric = dist_metric\n",
    "    def fit(self, X_train, y_train):\n",
    "        ... # Logic to fit the data\n",
    "    def predict(self, X_test):\n",
    "        neighbors = []\n",
    "        for x in X_test:\n",
    "            distances = self.dist_metric(x, self.X_train)\n",
    "            y_sorted = ... # Sort the values\n",
    "            neighbors.append(y_sorted[:self.k])\n",
    "        return list(map(mode, neighbors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ... # define a model from the above class with k=3\n",
    "... # fit the model \n",
    "y_pred = ... # predict the value from the model for  X_test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot The confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, title='Confusion matrix For KNN Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer the following\n",
    "1. Can we choose any value for K ?\n",
    "2. What will happen if we keep on increasing K ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparision of accuracy with different K value.\n",
    "# Load the model and plot the accuracies for different values of k \n",
    "ks = range(1, 10)\n",
    "accuracies = [] \n",
    "for k in ks:\n",
    "    ...\n",
    "    ...\n",
    "# Append array accuracies with different values of k\n",
    "\n",
    "\n",
    "# Accuracy vs. K\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ks, accuracies)\n",
    "ax.set(xlabel=\"k\",\n",
    "       ylabel=\"Accuracy\",\n",
    "       title=\"Performance of knn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "\n",
    "Implement for other distances and compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Using standard library\n",
    "\n",
    "Implement using Sklearn standard library and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and define Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = ... # define a model from the above class with k=3\n",
    "... # fit the model \n",
    "y_pred = ... # predict the value from the model for  X_test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot The confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, title='Confusion matrix For KNN Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the Following\n",
    "What is Deceision Tree and how do you implement it?   \n",
    "What is entropy in Decision Tree ?  \n",
    "What is information gain in Decision Tree ?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement your own Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.max_depth=max_depth\n",
    "        self.n_features=n_features\n",
    "        self.root=None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # check the stopping criteria\n",
    "        if (...): # Condition for Stopping Criteria\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        # find the best split\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        # create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        ...\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        # parent entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # calculate the weighted avg. entropy of children\n",
    "        ...\n",
    "\n",
    "        # calculate the IG\n",
    "        information_gain = ...\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        ...\n",
    "\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and define Model\n",
    "model = ... # define a model from the above class\n",
    "... # fit the model \n",
    "y_pred = ... # predict the value from the model for  X_test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot The confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, title='Confusion matrix For Naive Bayes Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Using standard library\n",
    "\n",
    "Implement using Sklearn standard library and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and define Model\n",
    "model = ... # define a model from the above imported model\n",
    "... # fit the model \n",
    "y_pred = ... # predict the value from the model for  X_test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot The confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, title='Confusion matrix For Naive Bayes Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
