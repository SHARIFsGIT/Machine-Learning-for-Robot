{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Ensemble Learning?\n",
    "\n",
    "- Ensemble methods combine several trees base algorithms to construct better predictive performance than a single tree base algorithm. \n",
    "\n",
    "- The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner, thus increasing the accuracy of the model. \n",
    "\n",
    "- When we try to predict the target variable using any machine learning technique, the main causes of difference in actual and predicted values are noise, variance, and bias. Ensemble helps to reduce these factors (except noise, which is irreducible error).\n",
    "\n",
    "<img src=\"1.png\">\n",
    "\n",
    "- As you see in the picture here it combaine 4 diffenet algorithm , and by this 4 algorithm it predict the output. Now you can imagine here it combaine diffenet algorithm and predict so it more powerful than single model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let's Dicuss About All Types Of Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Ensemble\n",
    "1. Max Voting\n",
    "\n",
    "2. Averaging\n",
    "\n",
    "3. Weighted Average\n",
    "\n",
    "## 1. Max Voting\n",
    "\n",
    "- The max voting method is generally used for classification problems. \n",
    "- In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict(x_test)\n",
    "pred2=model2.predict(x_test)\n",
    "pred3=model3.predict(x_test)\n",
    "\n",
    "final_pred = np.array([])\n",
    "for i in range(0,len(x_test)):\n",
    "    final_pred = np.append(final_pred, mode([pred1[i], pred2[i], pred3[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Averaging\n",
    "\n",
    "- Similar to the max voting technique, multiple predictions are made for each data point in averaging. \n",
    "\n",
    "- In this method, we take an average of predictions from all the models and use it to make the final prediction. Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.\n",
    "\n",
    "For example, in the below case, the averaging method would take the average of all the values.\n",
    "\n",
    "i.e. (5+7+8+3+5+4)/6 = 5.33"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(x_test)\n",
    "pred2=model2.predict_proba(x_test)\n",
    "pred3=model3.predict_proba(x_test)\n",
    "\n",
    "finalpred=(pred1+pred2+pred3)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weighted Average\n",
    "\n",
    "- This is an extension of the averaging method. \n",
    "- All models are assigned different weights defining the importance of each model for prediction. \n",
    "- For instance, if two of your colleagues are critics, while others have no prior experience in this field, then the answers by these two friends are given more importance as compared to the other people.\n",
    "\n",
    "The result is calculated as [(5*0.23) + (4*0.23) + (5*0.18) + (4*0.18) + (4*0.18)] = 4.41."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(x_test)\n",
    "pred2=model2.predict_proba(x_test)\n",
    "pred3=model3.predict_proba(x_test)\n",
    "\n",
    "finalpred=(pred1*0.3+pred2*0.3+pred3*0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Ensemble techniques\n",
    "\n",
    "- Now Dicuss Adance Ensemble Techniques\n",
    " 1. Bagging\n",
    " 2. Boosting\n",
    " 3. stacking\n",
    " 4. Blending\n",
    " 5. cascadeing\n",
    " \n",
    " \n",
    " \n",
    "### 1. Bagging\n",
    "- It is also know as Bootstrapped Aggregation\n",
    "- Bagging is mostly used to reduce the variance in a model. A simple example of bagging is the Random Forest algorithm.\n",
    "-  Bagging is one of the earliest, interesting and a very powerful ensemble algorithm. \n",
    "- The core idea of bagging is to use bootstrapped replicas of the original dataset and use them to train different classifiers.\n",
    "- Bagging (or Bootstrap Aggregating) technique uses these subsets (bags) to get a fair idea of the distribution (complete set). The size of subsets created for bagging may be less than the original set.\n",
    "\n",
    "#### How Bagging works on training dataset ?\n",
    "\n",
    " Since Bagging resamples the original training dataset with replacement, some instance(or data) may be present multiple times while others are left out.\n",
    " \n",
    "    Original training dataset: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "\n",
    "    Resampled training set 1: 2, 3, 3, 5, 6, 1, 8, 10, 9, 1\n",
    "    Resampled training set 2: 1, 1, 5, 6, 3, 8, 9, 10, 2, 7\n",
    "    Resampled training set 3: 1, 5, 8, 9, 2, 10, 9, 7, 5, 4\n",
    "    \n",
    " Its Process Flow:\n",
    " <img src=\"2.jpg\">\n",
    " \n",
    " Eg. Random Forest Classifier\n",
    " \n",
    " \n",
    " \n",
    " ### 2. Boosting\n",
    "\n",
    "- The second ensemble technique that we are going to discuss today is called Boosting.\n",
    "- Boosting is used to convert weak base learners to strong ones. Weak learners generally have a very weak correlation with the true class labels and strong learners have a very high correlation between the model and the true class labels.\n",
    "\n",
    "- AdaBoost was the first really successful boosting algorithm developed for the purpose of binary classification. AdaBoost is short for Adaptive Boosting and is a very popular boosting technique which combines multiple “weak classifiers” into a single “strong classifier”.\n",
    "\n",
    "<img src=\"3.jpg\">\n",
    "\n",
    "Eg. AdaBoost ,GBM,XGBM,Light GBM,CatBoost\n",
    "\n",
    "\n",
    "\n",
    "### Similarities Between Bagging and Boosting –\n",
    "\n",
    "* Both are ensemble methods to get N learners from 1 learner.\n",
    "* Both generate several training data sets by random sampling.\n",
    "* Both make the final decision by averaging the N learners (or taking the majority of them i.e Majority Voting).\n",
    "* Both are good at reducing variance and provide higher stability.\n",
    "\n",
    "\n",
    "### Where You Use bagging And Boosting\n",
    "\n",
    "- If the difficulty of the single model is over-fitting, then Bagging is the best option.\n",
    "- If the problem is that the single model gets a very low performance, Boosting could generate a combined model with lower errors as it optimises the advantages and reduces pitfalls of the single model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Stacking\n",
    "\n",
    "- Stacking is a way to ensemble multiple classifications or regression model.\n",
    "- Stacking is an ensemble learning technique which is used to combine the predictions of diverse classification models into one single model also known as the meta-classifier.\n",
    "- Stacking (sometimes called Stacked Generalization) is a different paradigm. The point of stacking is to explore a space of different models for the same problem.\n",
    "- The idea is that you can attack a learning problem with different types of models which are capable to learn some part of the problem, but not the whole space of the problem. So, you can build multiple different learners and you use them to build an intermediate prediction, one prediction for each learned model.\n",
    "- Then you add a new model which learns from the intermediate predictions the same target.\n",
    "- This final model is said to be stacked on the top of the others, hence the name. Thus, you might improve your overall performance, and often you end up with a model which is better than any individual intermediate model. Notice however, that it does not give you any guarantee, as is often the case with any machine learning technique.\n",
    "\n",
    "Stacking Procedure\n",
    "<img src=\"4.png\" width=800>\n",
    "\n",
    "### How stacking works?\n",
    "\n",
    "- We split the training data into K-folds just like K-fold cross-validation.\n",
    "- A base model is fitted on the K-1 parts and predictions are made for Kth part.\n",
    "- We do for each part of the training data.\n",
    "- The base model is then fitted on the whole train data set to calculate its performance on the test set.\n",
    "- We repeat the last 3 steps for other base models.\n",
    "- Predictions from the train set are used as features for the second level model.\n",
    "- Second level model is used to make a prediction on the test set.\n",
    "\n",
    "Eg.  Voting Classifier\n",
    "\n",
    "### 4.Belnding\n",
    "\n",
    "  Blending is a similar approach to stacking.\n",
    "\n",
    "- Blending is technique where we can do weighted averaging of final result.\n",
    "- Blending is also an ensemble technique that can help us to improve performance and increase accuracy. It follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions.\n",
    "- Here is a detailed explanation of the blending process:\n",
    "    - The train set is split into training and validation sets.\n",
    "    - We train the base models on the training set.\n",
    "    - We make predictions only on the validation set and the test set.\n",
    "    - The validation predictions are used as features to build a new model.\n",
    "    - This model is used to make final predictions on the test set using the prediction values as features.\n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    "- The difference between stacking and blending is that Stacking uses out-of-fold predictions for the train set of the next layer (i.e meta-model), and Blending uses a validation set (let’s say, 10-15% of the training set) to train the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cascading\n",
    "\n",
    "- Cascading is one of the most powerful ensemble learning algorithm which is used by Machine Learning engineers and scientists when they want to be absolutely dead sure about the accuracy of a result.\n",
    "- For example, suppose we want to build a machine learning model which would detect if a credit card transaction is fraudulent or not.\n",
    "- If you think about it, it’s a binary classification problem where a class label 0 means the transaction is not fraud & a class label 1 means the transaction is fraudulent. \n",
    "- In such a model, it’s very risky to put our faith completely on just one model. So what we do is build a sequence of models (or a cascade of models) to be absolutely sure about the fact that the transaction is not fraudulent.\n",
    "- Cascade models are mostly used when the cost of making a mistake is very very high. I will try to explain cascading with the help of a simple diagram.\n",
    "\n",
    "<img src=\"5.jpeg\">\n",
    "\n",
    "Look at the above diagram. Given that we have a transaction query point Xq, we will feed it to Model 1. Model 1 can be anything a random forest, or a logistic regression model or maybe a support vector machine. It can be anything! Basically what Model 1 does is that it predicts class probabilities to determine to which class do a given query point has higher chances of belonging to.\n",
    "\n",
    "Let’s say class label 1 means the transaction is fraudulent, and class label 0 means the transaction is not fraud. Typically, the predicted probabilities is given by this — P(Yq=0) and P(Yq=1), where Yq is our actual class label. Now let’s assume that P(Yq=0), i.e. the probability of the transaction to be not fraudulent is very high. If you think carefully, if P(Yq=0) is extremely high, we will say that the transaction is not fraud. Let’s assume we have set a threshold of 99%. It means if and only if P(Yq=0) > 0.99, we will declare the final prediction to be not fraudulent. However, if P(Yq=0) < 0.99 we are not very sure if or not it’s a fraudulent transaction although though there is a high chance that the transaction is not fraudulent. In such a case, when P(Yq=0) < 0.99, we want to be really really sure that the transaction is not fraudulent. We need to be absolutely careful because if our model fails to detect a fraudulent transaction we might lose millions of dollars!\n",
    "So even when we are slightly unsure, we will train another Model 2. Model 2 does the same thing, it receives the query point and predicts P(Yq=0). Just like in stage 1, if P(Yq=0) > 0.99, we will declare the transaction to be not fraudulent and terminate the loop. But again if we get P(Yq=0) < 0.99, we aren’ sure! Hence, we will pass the query point to another Model 3 in the cascade which does the same thing.\n",
    "\n",
    "\n",
    "In a typical cascading system the complexity of models increases as we add more and more models to the cascade. Please note that all the models in a cascade are super powerful and has a very high accuracy on unseen data. However, it might happen that none of the models can give us a value of P(Yq=0) > 0.99. In such a case, typically there is a human being who sits at the end of a cascade. This person will personally call the customer and ask him whether or not he has done the transaction. Now, we are absolutely certain that the transaction is not a fraud one when the customer says that he is the one who has done the transaction.\n",
    "\n",
    "@@ Credit:- https://medium.com/@saugata.paul1010/ensemble-learning-bagging-boosting-stacking-and-cascading-classifiers-in-machine-learning-9c66cb271674"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
