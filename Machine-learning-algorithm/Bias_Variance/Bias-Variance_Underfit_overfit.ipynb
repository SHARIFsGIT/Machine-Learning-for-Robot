{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Bias ?\n",
    "\n",
    "- It is  difference between Predicted Value and Actual Value.\n",
    "- Bias refers to the error introduced due to assumptions made to model a relatively complex real-life problem by an approximate simple model.\n",
    "\n",
    "\n",
    "\n",
    "  - Low Bias: Predicting less assumption about Target Function(Predict Value  >>  Actual Value)\n",
    "              -  difference is Very Samll Between predcit and Actual Value\n",
    "  \n",
    "  - High Bias: Predicting more assumption about Target Function (Predict Value  >>>>>>  Actual Value)\n",
    "              -  difference is very much large between predcit and Actual Value\n",
    "              \n",
    "\n",
    "Examples of low-bias machine learning algorithms include Decision Trees, k-Nearest Neighbors and Support Vector Machines.\n",
    "\n",
    "Examples of high-bias machine learning algorithms include Linear Regression, Linear Discriminant Analysis, and Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Variance ?\n",
    "\n",
    "- Variance is the amount that the estimate of the target function will change if different training data was used.\n",
    "- It determine how spread of predcit value each other\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       - Low Variance: Predicting small changes to the estimate of the target function with changes to the training dataset.\n",
    "\n",
    "\n",
    "      - High Variance: Predicting large changes to the estimate of the target function with changes to the training dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Examples of low-variance machine learning algorithms include Linear Regression, Linear Discriminant Analysis, and Logistic Regression.\n",
    "- Examples of high-variance machine learning algorithms include Decision Trees, k-Nearest Neighbors and Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Till now we have some basic knowledge about Bias and Variance, now let's know some other important terminology used in Bias and Variance that are Underfitting and Overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit And Underfit :\n",
    "\n",
    "<img src=\"1.png\" width=500>\n",
    "\n",
    "\n",
    "\n",
    "## Underfit :\n",
    "\n",
    "- These models usually have high bias and low variance. \n",
    "- underfitting happens when a model unable to capture the underlying pattern of the data. \n",
    "- It happens when we have very less amount of data to build an accurate model or when we try to build a linear model with a nonlinear data. Also, these kind of models are very simple to capture the complex patterns in data like Linear and logistic regression.\n",
    "\n",
    "- Here Training Accuracy is 50% But Testing Accuracy is around 49%. See there is less accuracy this tends to underfit.\n",
    "\n",
    "- Underfitting is when the model’s error on both the training and test sets (i.e. during training and testing) is very high.\n",
    "\n",
    "####  How to Oercome Underfitting :\n",
    "\n",
    "- In that case, there are 2 gold standard approaches:\n",
    "\n",
    "        1. Try another model\n",
    "        2. Increase the complexity of the current model\n",
    "        \n",
    "Solution 1 is trivial. Concerning solution 2, an example an be the following: if someone is fitting a linear regression to some data, then increasing the complexity would mean to fit a polynomial model.\n",
    "\n",
    "\n",
    "## Overfit :\n",
    "\n",
    "- These models have low bias and high variance.\n",
    "- overfitting happens when our model captures the noise along with the underlying pattern in data.\n",
    "- It happens when we train our model a lot over the noisy dataset. \n",
    "- These models are very complex like Decision trees which are prone to overfitting.\n",
    "\n",
    "\n",
    "- - Here Training Accuracy is 99% But Testing Accuracy is around 50%. there is huge difference between training and testing score this tends to overfit\n",
    "\n",
    "- Overfitting is when the model’s error on the training set (i.e. during training) is very low but then, the model’s error on the test set (i.e. unseen samples) is large!\n",
    "\n",
    "\n",
    "####  How to overcome Overfitting :\n",
    "\n",
    "- The most common problem in the ML learning filed is overfitting.\n",
    "\n",
    "- Action that could (potentially) limit overfitting:\n",
    "        \n",
    "        1. We can use a Cross-validation (CV) scheme.\n",
    "        \n",
    "        2. Reduce the complexity of the model (make the model less complex).\n",
    "\n",
    "- When it comes to solution 1 i.e. the use of cross-validation, the most famous CV scheme is the KFolds cross-validation. Using a KFolds scheme, we train and test your model k-times on different subsets of the training data and estimate a performance metric using the test (unseen) data.\n",
    "\n",
    "- When it comes to solution 2 i.e. reducing the complexity of the model can help reduce the overfitting. For example, if someone is using an SVM model with RBF kernel then reducing the complexity would mean to use a linear kernel. In another case, if someone is fitting a polynomial to some data, then reducing the complexity would mean to fit a linear model instead (linear regression).\n",
    "\n",
    "\n",
    "<img src = \"3.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Bias-Variance Tradeoff ?\n",
    "\n",
    "- If our model is too simple and has very few parameters then it may have high bias and low variance. \n",
    "- On the other hand, if our model has a large number of parameters then it’s going to have high variance and low bias.\n",
    "- So we need to find the right/good balance without overfitting and underfitting the data.\n",
    "\n",
    "\n",
    "\n",
    "- There is no escaping the relationship between bias and variance in machine learning.\n",
    "        * Increasing the bias will decrease the variance.\n",
    "        * Increasing the variance will decrease the bias.\n",
    "        \n",
    "- If the algorithm is too simple  then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as Trade-off or Bias Variance Trade-off.\n",
    "\n",
    "- The error to complexity graph to show trade-off is given as –\n",
    "\n",
    "<img src=\"2.png\" width=500>.\n",
    "\n",
    "\n",
    "The above picture shows that :\n",
    "\n",
    "- Bias initially decreases faster than variance.\n",
    "- After a point, variance increases significantly with an increase in flexibility with little impact on bias.\n",
    "\n",
    "This is referred to as the best point chosen for the training of the algorithm which gives low error in training as well as testing data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
