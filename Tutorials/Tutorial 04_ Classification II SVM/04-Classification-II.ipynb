{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 04: Classification II SVM\n",
    "\n",
    "In this notebook, we perform classification of Robot movements in an environment with walls using real dataset. \n",
    "\n",
    "Install the necessary libraries in the PC or in the Virtual Environment using provided Requirements.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import cvxopt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from functions.plot_confusion_matrix import plot_confusion_matrix\n",
    "\n",
    "cvxopt.solvers.options['show_progress'] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Simple SVM Classifier\n",
    "\n",
    "Illustration of SVM:\n",
    "\n",
    "1. Seperation Problem\n",
    " \n",
    "<img src=\"figures/separation_problem.jpg\">\n",
    "\n",
    "2. SVM Classification\n",
    "\n",
    "<img src=\"figures/svm-all.jpg\">    \n",
    " \n",
    "\n",
    "\n",
    "Source: https://www.baeldung.com/\n",
    "\n",
    "Answer the following:\n",
    "\n",
    "1. What is SVM and why do we need it?   \n",
    "2. What is the basic idea of SVM?   \n",
    "3. What are the support vectors?   \n",
    "4. What is hyperplane?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a simple SVM Classifier for binary classification of data.\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = ...\n",
    "                if condition:\n",
    "                    self.w -= ...\n",
    "                else:\n",
    "                    self.w -= ...\n",
    "                    self.b -= ...\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = ...\n",
    "        return np.sign(approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=50, n_features=2, centers=2, cluster_std=1.05, random_state=40)\n",
    "y = np.where(y == 0, -1, 1)\n",
    "\n",
    "model = SVM()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "print(model.w, model.b)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperplane_value(x, w, b, offset):\n",
    "    return (-w[0] * x + b + offset) / w[1]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y)\n",
    "\n",
    "x0_1 = np.amin(X[:, 0])\n",
    "x0_2 = np.amax(X[:, 0])\n",
    "\n",
    "x1_1 = get_hyperplane_value(x0_1, model.w, model.b, 0)\n",
    "x1_2 = get_hyperplane_value(x0_2, model.w, model.b, 0)\n",
    "\n",
    "x1_1_m = get_hyperplane_value(x0_1, model.w, model.b, -1)\n",
    "x1_2_m = get_hyperplane_value(x0_2, model.w, model.b, -1)\n",
    "\n",
    "x1_1_p = get_hyperplane_value(x0_1, model.w, model.b, 1)\n",
    "x1_2_p = get_hyperplane_value(x0_2, model.w, model.b, 1)\n",
    "\n",
    "ax.plot([x0_1, x0_2], [x1_1, x1_2], \"y--\")\n",
    "ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], \"k\")\n",
    "ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], \"k\")\n",
    "\n",
    "x1_min = np.amin(X[:, 1])\n",
    "x1_max = np.amax(X[:, 1])\n",
    "ax.set_ylim([x1_min - 3, x1_max + 3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Non-Linear SVM Classifier\n",
    "1. Is above data linearly seperable ? \n",
    "2. How to deal with non-linear seperable data ? \n",
    "3. What is kernel trick and how can we use it in SVM ?\n",
    "\n",
    "Kernel Trick:   \n",
    "\n",
    "<img src=\"figures/Kernel_Trick_01.jpg\">\n",
    "<img src=\"figures/Kernel_Trick_02.jpg\">\n",
    "<img src=\"figures/Kernel_Trick_03.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = datasets.load_iris()\n",
    "X = normalize((data.data[data.target != 0]), norm='l2')\n",
    "y = data.target[data.target != 0]\n",
    "y[y == 1] = -1\n",
    "y[y == 2] = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define kernel functions\n",
    "def linear_kernel(**kwargs):\n",
    "    def f(x1, x2):\n",
    "        return ...\n",
    "    return f\n",
    "\n",
    "\n",
    "def polynomial_kernel(power, coef, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        return ...\n",
    "    return f\n",
    "\n",
    "\n",
    "def rbf_kernel(gamma, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        distance = np.linalg.norm(x1 - x2) ** 2\n",
    "        return ...\n",
    "    return f\n",
    "\n",
    "class SupportVectorMachine():\n",
    "    def __init__(self, C=1, kernel=rbf_kernel, power=4, gamma=None, coef=4):\n",
    "        self.C = C\n",
    "        self.power = power\n",
    "        self.gamma = gamma\n",
    "        self.coef = coef\n",
    "        self.kernel = kernel\n",
    "        self.lagr_multipliers = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "        if self.gamma == None:\n",
    "            self.gamma = 1 / n_features\n",
    "\n",
    "        self.kernel = self.kernel(power=self.power,gamma=self.gamma,coef=self.coef)\n",
    "\n",
    "        # Calculate kernel matrix\n",
    "        kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                ...\n",
    "\n",
    "        # Define the quadratic optimization problem\n",
    "        P = cvxopt.matrix(np.outer(y, y) * kernel_matrix, tc='d')\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1, n_samples), tc='d')\n",
    "        b = cvxopt.matrix(0, tc='d')\n",
    "\n",
    "        if not self.C:\n",
    "            G = cvxopt.matrix(np.identity(n_samples) * -1)\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            G_max = np.identity(n_samples) * -1\n",
    "            G_min = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((G_max, G_min)))\n",
    "            h_max = cvxopt.matrix(np.zeros(n_samples))\n",
    "            h_min = cvxopt.matrix(np.ones(n_samples) * self.C)\n",
    "            h = cvxopt.matrix(np.vstack((h_max, h_min)))\n",
    "\n",
    "        # Solve the quadratic optimization problem using cvxopt\n",
    "        minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        lagr_mult = ...\n",
    "\n",
    "        # Extract support vectors\n",
    "        # Get indexes of non-zero lagr. multipiers\n",
    "        idx = ...\n",
    "        # Get the corresponding lagr. multipliers\n",
    "        self.lagr_multipliers = ...\n",
    "        # Get the samples that will act as support vectors\n",
    "        self.support_vectors = ...\n",
    "        # Get the corresponding labels\n",
    "        self.support_vector_labels = ...\n",
    "\n",
    "        # Calculate intercept with first support vector\n",
    "        self.intercept = self.support_vector_labels[0]\n",
    "        for i in range(len(self.lagr_multipliers)):\n",
    "            self.intercept -= self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                i] * self.kernel(self.support_vectors[i], self.support_vectors[0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "\n",
    "        for sample in X:\n",
    "            prediction = 0\n",
    "            for i in range(len(self.lagr_multipliers)):\n",
    "                prediction += ...\n",
    "            prediction += self.intercept\n",
    "            y_pred.append(np.sign(prediction))\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SupportVectorMachine(kernel=polynomial_kernel, power=4, coef=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3: Multiclass SVM Classifier using Standard Library\n",
    "\n",
    "In basic form, SVM doesnot support Multiclass classification. For multiclass classification, the problem is subdivided into multiple binary classification problems. \n",
    "\n",
    "The popular methods which are used to perform multi-classification using SVM are as follows:\n",
    "\n",
    "1. One vs One (OVO) approach\n",
    "\n",
    "<img src=\"figures/multiclass-svm-1.jpg\">\n",
    "\n",
    "2. One vs All (OVA) approach\n",
    "\n",
    "<img src=\"figures/multiclass-svm-2.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from csv to Pandas Dataframe\n",
    "\n",
    "data = np.loadtxt(\"Data/sensor_readings_24.csv\", delimiter=',', dtype=str)\n",
    "\n",
    "df = pd.DataFrame(data[:,:24], dtype=np.float64)\n",
    "df = pd.concat([df, pd.DataFrame(data[:, 24], columns=['Label'])], axis=1)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "# generate dataset\n",
    "\n",
    "# define feature selection\n",
    "fs = SelectKBest(score_func=f_classif, k=3)\n",
    "# apply feature selection\n",
    "df_selected_1 = fs.fit_transform(df.iloc[:, 0:24], df['Label'])\n",
    "# Test and Train data splitting\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "labelEn = LabelEncoder()\n",
    "encoded_labels = ...\n",
    "class_names = ...\n",
    "\n",
    "X_train, X_test, y_train, y_test = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# Apply SVM Classifier for rbf kernel\n",
    "model = ...\n",
    "model.fit(...)\n",
    "y_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot The confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, title='Confusion matrix For SVM Classification with rbf kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVM Classifier for Polynomial kernel\n",
    "model = ...\n",
    "model.fit(...)\n",
    "y_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot The confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, title='Confusion matrix For KNN Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
