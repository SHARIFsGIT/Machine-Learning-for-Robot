{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe43da90",
   "metadata": {},
   "source": [
    "# Tutorial: Features and Preprocessing\n",
    "\n",
    "In this notebook, we will implement the Naive Feature Selector algorithm from the lecture and apply it on a real dataset. Let's start off by importing everything we need for this tutorial. Please do not use any other imports for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a477ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f974b20",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Let's begin by loading the dataset and exploring it. Run the cell below and then inspect the data, which is provided as a [Python dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries). Use the `print()` and `dict.keys()` functions to see its content and the answer the following questions:\n",
    "\n",
    "    1. How many samples does the dataset contain?\n",
    "    2. How many features does each instance consist of?\n",
    "    3. Is the data labeled (and if so, how many classes are there)?\n",
    "    4. What are the names of the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad66be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the data\n",
    "cancer_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84db38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect the data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d1c3d8",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Now, let's implement the naive feature selector. Complete the methods stub below and use it to select the three features with the strongest feature-class correlation. What are their names?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dbaa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_feature_selector(X: np.ndarray, y: np.ndarray, k: int) -> list[int]:\n",
    "    # YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691f11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73918e31",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Now, we want to try out different values for k. First split the data into a training set and a test set. The test set should consist of 50 instances. Then create a for loop to select k values and train a classifier and predict the performance on the test samples using the provided black box classifier. Don't worry, we'll learn much more about the used classfication method over the course of the semester ;) Store the classification results for k-values ranging from 1 to 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and test sets\n",
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# we'll consider this a blackbox classifier today\n",
    "def train_and_predict(feature_indices: list[int]) -> float:\n",
    "    \n",
    "    # train the classifier\n",
    "    classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "    classifier.fit(X_train[:, features], y_train)\n",
    "    \n",
    "    # predict using trained classifier\n",
    "    pred = classifier.predict(X_test[:, features])\n",
    "    \n",
    "    # return the score\n",
    "    return np.mean(pred==y_test) \n",
    "\n",
    "#YOUR CODE GOES HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c55959",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Finally, visualise your results using matplotlib. Make sure to set a title for your figure and label your axes. How do you explain the shape of your plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d63c7cf",
   "metadata": {},
   "source": [
    "### Bonus Task\n",
    "Also implement the calculation of merit and the GreedyCFS algorithm from the lecture. Then rerun the experiments above and compare them visually in your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac942c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
